{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Continuous Control Agents with Evolutionary Algorithms\n",
    "\n",
    "### Author:  `Andrew Malta`\n",
    "\n",
    "## Introduction\n",
    "In this project, I explore the application of evolutionary algorithms to learn policies for continuous control agents in the OpenAI Gym.  The OpenAI Gym, while traditionally used to measure the performance of reinforcement learning approaches on toy environments from Atari games to motion tasks, provided me with an ideal framework to test the performance of evolutionary algorithms on learning deterministic policies for agents. I first got interested in this topic when I read OpenAI's paper, \"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\", which experimentally demonstrated that it is possible to rival the performance of reinforcement learning in many tasks while gaining the desirable property of having highly parallelizable training procedures.  This report along with an excellent blog post by David Ha about applying these techniques to the OpenAI gym in particular, both referenced at the bottom, encouraged me to try my hand at some of the hardest control tasks offered in the OpenAI gym. \n",
    "\n",
    "\n",
    "## Environments\n",
    "In choosing an environment to experiment with, I first looked for a task that would be particularly amenable to reinforcement learning, as I wanted to see that a typical reinforcment learning task could be solved through the use of an evolutionary algorithm.  I settled on the Bipedal walking task in the OpenAI gym as it seemed to meet  these criteria and due to the fact that it had both a normal and a hardcore version, which I thought I might be able to exploit in the training process. Very simply, the goal of the task is to have your agent navigate itself across the terrain in front of it and reach the goal on the other end.  The reward function penalizes stumbling, defined to be when the head hits the ground, and the use of excess motor torque.  A good solution is robust to the terrain, but does not use too much motor torque to maintain this robustness. \n",
    "\n",
    "The input to the model is a stream of 24 dimensional vectors each of which includes hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, leg contacts with the ground, and 10 lidar inputs that encode objects in the vicinity. The output is a 4 dimensional vector representing the amount of torque to apply to each of the 4 joints on the agent.\n",
    "\n",
    "The normal version of the environment has small random variation in the terrain, but all in all it is pretty flat.\n",
    "\n",
    "![Random Agent on BipedalWalker-v2](https://media.giphy.com/media/NVmoxc9Jjri0r8D6fz/giphy.gif)\n",
    "\n",
    "The hardcore version of the environment, on the other hand, has pits, ramps, and obstructions\n",
    "of various sizes that the agent has to navigate.  Due to this terrain, it is much\n",
    "easier for the agent to get stuck in local minima, as it can learn how to navigate some of the terrain but not the rest.\n",
    "\n",
    "![Random Agent on BipedalWalkerHardore-v2](https://media.giphy.com/media/1qk0RMLewHC6LnYPI1/giphy.gif)\n",
    "\n",
    "The hardcore task poses a major leap in difficulty as the terrain varies much more significantly for each run of the environment than the normal version of the environment does.  This forces you to learn a robust policy to work in any configuration of these ramps, pits, and obstructions rather than learn a\n",
    "particular motor sequence that works for one specific instantiation environment.  This was not a major issue in the normal bipedal walking environment as the terrain variation was minimal, allowing the evolutionary algorithm to simply learn a motor sequence that propelled itself forward while keeping its balance.  \n",
    "\n",
    "## Model\n",
    "For this project, I decided to roll my own small neural network framework as I wanted to have direct access to the parameterization of the weights and biases of the network through a single flat vector.  This, in my mind, was the easiest way to interface the evoltionary algorithms with my model as most libraries make you go out of your way to do something other than the classic back propagation with some optimizer.  I wanted to keep the model simple because I wanted to try keep the focus on the optimzation procedure, and I wanted the optimzation routine to run quickly.\n",
    "\n",
    "The model I ended up choosing was a feed-forward nerual network with 2 hidden layers and hypberbolic tangent activation, which ended up totaling a parameter vector of length 2804 when you account for the biases.  This number of parameters, of course, was a main concern when I was choosing the size of my model, as I wanted to ensure that the neural network was able to represent the function I was trying to approximate. That being said, I wanted to keep the parameter vector small enough for performance reasons, as I wanted to test the performance of the CMA-ES optimization algorithm, which scales poorly with the size of the parameter vector. The code for this simple neural network model is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Net():\n",
    "    def __init__(self, weights):\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.dimensions = [(24, 40), (40, 40), (40, 4)]\n",
    "        self.layers = []\n",
    "        self.biases = []\n",
    "        if not weights is None:\n",
    "            tmp = 0\n",
    "            for d in self.dimensions:\n",
    "                length = np.prod(d)\n",
    "                self.layers.append(np.reshape(weights[tmp: tmp + length], d))\n",
    "                tmp += length\n",
    "                self.biases.append(np.reshape(weights[tmp: tmp + d[-1]], (1, d[-1])))\n",
    "                tmp += d[-1]\n",
    "\n",
    "    def set_model_params(self, weights):\n",
    "        self.layers = []\n",
    "        self.biases = []\n",
    "\n",
    "        tmp = 0\n",
    "        for d in self.dimensions:\n",
    "            length = np.prod(d)\n",
    "            self.layers.append(np.reshape(weights[tmp: tmp + length], d))\n",
    "            tmp += length\n",
    "            self.biases.append(np.reshape(weights[tmp: tmp + d[-1]], (1, d[-1])))\n",
    "            tmp += d[-1]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        working_tensor = x\n",
    "        for i in xrange(len(self.layers)):\n",
    "            affine = np.dot(working_tensor, self.layers[i]) + self.biases[i]\n",
    "            working_tensor = np.tanh(affine)\n",
    "        return working_tensor[0]\n",
    "\n",
    "    def num_flat_features(self):\n",
    "        ret = 0\n",
    "        for d in self.dimensions:\n",
    "            ret += np.prod(d)\n",
    "            ret += d[-1]\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as the fitness function goes, I didn't add many bells and whistles aside from an optional parameter to allow the scores to be averaged across a specified number of trials.  I did this in an attempt to improve the robustness of the model, especially for the Bipedal Walker Hardcore environment as the agent could easily do well on one randomly generated terrain and poorly on another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate(env, model_cls, weights, num_trials, seed=None, render=False):\n",
    "    m = model_cls(weights)\n",
    "    rewards = []\n",
    "    for _ in xrange(num_trials):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        best_reward = -float(\"inf\")\n",
    "        total = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = m.forward(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            total += reward\n",
    "\n",
    "        rewards.append(total)\n",
    "\n",
    "    return np.mean(rewards), range(num_trials)\n",
    "\n",
    "def evaluate(env, model_cls, weights):\n",
    "    return simulate(env, model_cls, weights, 100, None)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code snippet, we instantiate our model with the weights that are passed in and run a simulation using our network num_trials number of times.  We then take the average reward across these trials and return the result.  We also see the evaluate function, which gives you what your average score is over 100 trials, which is what the OpenAI gym uses to compare different approaches to the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "I played around with a number of evolutionary algorithms, some of which do not have a canonical name and others that certainly do.  For each algorithm I will give a little intuition about how to interpret what it is doing, provide pseudocode, and list my python 2 implementation of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask/Tell Interface\n",
    "One design pattern that I found incredibly useful both to think about the evolutionary algorithms conceptually, and to implement them in python was the ask/tell interface that was described in David Ha's second blog post describing ESTool, \"Evolving Stable Strategies\".  Conceptually, it is very simple.  The algorithm exposes an **ask** function that will supply the user with new parameter vectors that are sampled in some way from the current state of the algorithm.  The user then takes these new parameter vectors and computes the fitness score for each of them.  After the user completes the fitness evaluations, the user calls the algorithms **tell** function, which supplies the algorithm with the fitnesses of the parameter vectors that it supplied the user.  The algorithm then uses this information to update any internal state of the algorithm to inform the next iteration of the ask/tell interface.\n",
    "\n",
    "Outside of improvements to code readability, this paradigm allows the programmer to abstract away how the fitnesses are being computed.  More specifically, it allows the programmer to decide if they want to parallelize this computation, which they often might, without having the parallelization code to have to touch the implementation of the optimization routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WinnerES\n",
    "WinnerES is a basic template for a evolutionary algorithm with an update step in which it chooses the best child from the generation and uses it as the starting point of the next generation.  This is not great for a number of reasons, but it works for some simple tasks and provides a simplified view of how an evolutionary algorithm works.  \n",
    "\n",
    "If you wanted to interpret it slightly more mathematically, this algorithm can be seen as taking a randomized sampling of directions on the objective function to approximate the gradient at your current parameter vector.  The larger your generation size, n, the better your approximation of the gradient gets, but the longer the computation takes per step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for WinnerES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "\\begin{align}\n",
       "    &\\theta_{0} = \\vec{0} \\\\\n",
       "    &\\text{for some number of generations:} \\\\\n",
       "        &\\hspace{1cm} \\epsilon_{1,...,n} \\sim N(\\sigma, I) \\\\\n",
       "        &\\hspace{1cm}  j = \\text{argmax}_{i} ~ f(\\theta_t + \\epsilon_i) \\\\\n",
       "        &\\hspace{1cm}  \\theta_{t+1} = \\theta_{t} + \\epsilon_j \\\\\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "\\begin{align*}\n",
    "    &\\theta_{0} = \\vec{0} \\\\\n",
    "    &\\text{for some number of generations:} \\\\\n",
    "        &\\hspace{1cm} \\epsilon_{1,...,n} \\sim N(\\sigma, I) \\\\\n",
    "        &\\hspace{1cm}  j = \\text{argmax}_{i} ~ f(\\theta_t + \\epsilon_i) \\\\\n",
    "        &\\hspace{1cm}  \\theta_{t+1} = \\theta_{t} + \\epsilon_j \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of WinnerES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Winner_ES:\n",
    "    def __init__(self, kwargs):\n",
    "        self.theta_dim = kwargs[\"theta_dim\"]\n",
    "        self.gen_size = kwargs[\"gen_size\"]\n",
    "        self.sigma = kwargs[\"sigma\"]\n",
    "        self.theta = np.zeros(self.theta_dim)\n",
    "\n",
    "    def ask(self):\n",
    "        self.epsilon = np.random.randn(self.gen_size, self.theta_dim)\n",
    "        self.solutions = self.theta.reshape(1, self.theta_dim) + self.epsilon * self.sigma\n",
    "\n",
    "        return self.solutions\n",
    "\n",
    "    def tell(self, fitnesses):\n",
    "        argsort = list(np.argsort([-val for val in fitnesses]))\n",
    "        self.theta = self.solutions[argsort[0]]\n",
    "\n",
    "    def result(self):\n",
    "        return self.theta\n",
    "\n",
    "    def set_theta(self, theta):\n",
    "        self.theta = np.array(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Threaded OpenES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next algorithm I implemented and tested on some toy examples was the non-parallelized version of OpenAI's evolutionary strategy, which is described in detail in algorithm 1 in the paper, \"Evolution strategies as a scalable alternative to reinforcement learning\", listed in the references.  In my code I reference it as WeightedAverageES, and the updates are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for Single Threaded OpenES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "\\begin{align}\n",
       "    &\\theta_{0} = \\vec{0} \\\\\n",
       "    &\\text{for some number of generations:} \\\\\n",
       "      &\\hspace{1cm} \\epsilon){1,...,n} \\sim N(\\sigma, I)\\\\\n",
       "      &\\hspace{1cm} f_i = f(\\theta_t + \\epsilon_i) \\\\\n",
       "      &\\hspace{1cm} \\theta_{t+1} = \n",
       "        \\theta_t + \\frac{\\alpha}{n\\sigma} \\sum_{i}^n f_i * \\epsilon_i \\\\\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "\\begin{align*}\n",
    "    &\\theta_{0} = \\vec{0} \\\\\n",
    "    &\\text{for some number of generations:} \\\\\n",
    "      &\\hspace{1cm} \\epsilon){1,...,n} \\sim N(\\sigma, I)\\\\\n",
    "      &\\hspace{1cm} f_i = f(\\theta_t + \\epsilon_i) \\\\\n",
    "      &\\hspace{1cm} \\theta_{t+1} = \n",
    "        \\theta_t + \\frac{\\alpha}{n\\sigma} \\sum_{i}^n f_i * \\epsilon_i \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Single Threaded OpenES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Weighted_Average_ES:\n",
    "    def __init__(self, kwargs):\n",
    "        self.theta_dim = kwargs[\"theta_dim\"]\n",
    "        self.gen_size = kwargs[\"gen_size\"]\n",
    "        self.sigma = kwargs[\"sigma\"]\n",
    "        self.alpha = kwargs[\"alpha\"]\n",
    "        self.n = kwargs[\"n\"]\n",
    "        self.theta = np.zeros(self.theta_dim)\n",
    "\n",
    "    def ask(self):\n",
    "        self.epsilon = np.random.randn(self.gen_size, self.theta_dim)\n",
    "        self.solutions = self.theta.reshape(1, self.theta_dim) + self.epsilon * self.sigma\n",
    "\n",
    "        return self.solutions\n",
    "\n",
    "    def tell(self, fitnesses):\n",
    "        argsort = list(np.argsort([-val for val in fitnesses]))\n",
    "        fitness_update_sum = np.zeros(self.theta_dim)\n",
    "\n",
    "        for i in argsort[0:n]:\n",
    "            fitness_update_sum += (self.epsilon[i] * fitnesses[i])\n",
    "\n",
    "        self.theta = self.theta + (self.alpha / self.n * self.sigma) * fitness_update_sum\n",
    "\n",
    "    def result(self):\n",
    "        return self.theta\n",
    "\n",
    "    def set_theta(self, theta):\n",
    "        self.theta = np.array(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this algorithm is very similar in spirit to the basic evolutionary algorithm, WinnerES; however, rather than just updating the internal parameter vector $\\theta$ to be the best performing member of the population, it performs a weighted average of the best $n$ members of the populated weighted by their corresponding fitness score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMA-ES (Covariance Matrix Adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMA-ES is one of the more famous gradient-less optimization routines for non-convex optimization problems.  It too, is an evolutionary algorithm; however it has significantly more bells and whistles than previously presented algorithms.  The most salient difference, from which CMA gets its name, is the way that the algorithm handles pairwise interactions between parameters in the vector it optimizing over.  It operates by attempting to maximize the liklihood of sampling previously good candidates and fruitful search steps.  This algorithm does not scale extremely well with the length of the parameter vector, as one of the update steps involves inverting the covariance matrix $C$.  As of now, the best known algorihtms for computing an inverse of a matrix runs in $O(n^2.373)$ time, which will give us trouble when we approach n of about 10,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for CMA-ES\n",
    "<img src=\"https://i.imgur.com/GIcOyry.png\">\n",
    "\n",
    "Image Source: https://en.wikipedia.org/wiki/CMA-ES#Algorithm\n",
    "\n",
    "For a more in-depth description of the individual update steps, my implementation follows the implementation details of the algorithm described on Wikipedia at the link above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of CMA-ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CMA_ES:\n",
    "    def __init__(self, kwargs):\n",
    "        self.theta_dim = kwargs[\"theta_dim\"]\n",
    "        self.gen_size = kwargs[\"gen_size\"]\n",
    "        self.start_sigma = kwargs[\"start_sigma\"]\n",
    "        self.mu = kwargs[\"mu\"]\n",
    "\n",
    "        self.theta = np.zeros(self.theta_dim)\n",
    "\n",
    "        self.n = self.theta_dim  # parameter for ease of readability\n",
    "        self.lamb = self.gen_size # use same notation as wikipedia\n",
    "\n",
    "        # initialization of the 5 major parameters that we update\n",
    "        self.mean = np.zeros(self.n)\n",
    "        self.sigma = self.start_sigma\n",
    "        self.C = np.identity(self.n)\n",
    "        self.p_sigma = np.zeros(self.n)\n",
    "        self.p_c = np.zeros(self.n)\n",
    "\n",
    "    #TODO: Vectorize this code for perf\n",
    "    def ask(self):\n",
    "        self.solutions = []\n",
    "        for _ in xrange(self.gen_size):\n",
    "            self.solutions.append(np.random.multivariate_normal(\n",
    "                self.mean, self.sigma * self.sigma * self.C))\n",
    "\n",
    "        return self.solutions\n",
    "\n",
    "    def tell(self, fitnesses):\n",
    "        tmp = self.mean\n",
    "        sorted_indices = np.argsort(fitnesses)\n",
    "\n",
    "        # update m\n",
    "        self.mean = 0\n",
    "        # using equal weighting \n",
    "        for i in sorted_indices[0:self.mu]:\n",
    "            self.mean += self.solutions[i] / self.mu\n",
    "\n",
    "        # update p_sigma\n",
    "        c_sigma = 3. / self.n\n",
    "        df = 1 - c_sigma # discount factor\n",
    "        C_tmp = np.sqrt(np.linalg.inv(self.C))\n",
    "        displacement = (self.mean - tmp) / float(self.sigma)\n",
    "\n",
    "        self.p_sigma = (df * self.p_sigma + \n",
    "            np.sqrt(1 - np.power(df,2)) * np.sqrt(self.mu) * C_tmp * displacement)\n",
    "\n",
    "        # update p_c\n",
    "        c_c = 4. / self.n\n",
    "        df2 = (1 - c_c)\n",
    "        alpha = 1.5\n",
    "\n",
    "        norm = np.linalg.norm(self.p_sigma)\n",
    "        indicator = norm >= 0 and norm <= alpha * np.sqrt(self.n)\n",
    "        complements = np.sqrt(1 - np.power((1 - c_c), 2))\n",
    "        neutral_selection = np.sqrt(self.mu) * displacement\n",
    "        self.p_c = df2 * self.p_c + indicator * (complements * neutral_selection)\n",
    "\n",
    "        # update Covariance Matrix\n",
    "        c_1 = 2. / np.power(self.n, 2)\n",
    "        c_mu = self.mu / np.power(self.n, 2)\n",
    "        c_s = (1 - indicator)* c_1 * c_c * (2 - c_c)\n",
    "\n",
    "        tmp_pc = self.p_c.reshape(-1, 1)\n",
    "        rank_1_mat = c_1 * tmp_pc.dot(tmp_pc.transpose())\n",
    "\n",
    "        mat_sum = np.zeros(self.C.shape)\n",
    "        for i in xrange(self.mu):\n",
    "            vec = ((self.solutions[sorted_indices[i]] - tmp) / self.sigma).reshape(-1, 1)\n",
    "            mat_sum += (1. / self.mu) * vec.dot(vec.transpose())\n",
    "\n",
    "        self.C = (1 - c_1 - c_mu + c_s) * self.C + rank_1_mat + c_mu * mat_sum \n",
    "\n",
    "        # update sigma\n",
    "        d_sigma = 1  # dampening parameter\n",
    "        p_sigma_norm = np.linalg.norm(self.p_sigma)\n",
    "        tmp = (1 - 1. / (4 * self.n) + 1. / (21 * self.n * self.n))\n",
    "        nse = np.sqrt(self.n) * tmp  # neutral selection expectation\n",
    "        self.sigma = self.sigma * np.exp((c_sigma / d_sigma) * ((p_sigma_norm / nse) - 1))\n",
    "\n",
    "    def set_theta(self, theta):\n",
    "        self.mean = np.array(theta)\n",
    "\n",
    "    def result(self):\n",
    "        return self.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I implemented OpenAI's OpenES algorithm. I added some small additions that are common to evolutionary algorithms, such as a weight decay, an adaptive learning rate, and adaptive standard deviation for the mutation rate.  The basic algorithm is the same as we described before, however there is some added complexity in setting up the algorithm to work efficiently under the parallel setting.  This is the algorithm that I was able to get quick convergence with to the solutions found in the results section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of an Extended OpenES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' helper functions taken from OpenAI's implementation of their algorithm which\n",
    "can be found here:\n",
    "(https://github.com/openai/evolution-strategies-starter/blob/master/es_distributed/es.py)\n",
    "'''\n",
    "def compute_ranks(x):\n",
    "    \"\"\"\n",
    "    Returns ranks in [0, len(x))\n",
    "    Note: This is different from scipy.stats.rankdata, which returns ranks in [1, len(x)].\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1\n",
    "    ranks = np.empty(len(x), dtype=int)\n",
    "    ranks[x.argsort()] = np.arange(len(x))\n",
    "    return ranks\n",
    "\n",
    "def compute_centered_ranks(x):\n",
    "    \"\"\"\n",
    "    https://github.com/openai/evolution-strategies-starter/blob/master/es_distributed/es.py\n",
    "    \"\"\"\n",
    "    y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
    "    y /= (x.size - 1)\n",
    "    y -= .5\n",
    "    return y\n",
    "\n",
    "def compute_weight_decay(weight_decay_coef, theta):\n",
    "    \"\"\"\n",
    "    compute the weight decay for the model parameters. Used to encourage the \n",
    "    mean model parameter squared to not get too large. \n",
    "    \"\"\"\n",
    "    flattened = np.array(theta)\n",
    "    mean_sq_theta = np.mean(flattened * flattened, axis=1)\n",
    "    return -weight_decay_coef * mean_sq_theta\n",
    "\n",
    "class OpenES:\n",
    "    ''' Basic Version of OpenAI Evolution Strategy.'''\n",
    "    def __init__(self, args):\n",
    "        self.theta_dim = args[\"theta_dim\"]\n",
    "        self.sigma = args[\"sigma_start\"]\n",
    "        self.sigma_init = args[\"sigma_start\"]\n",
    "        self.sigma_lower_bound = args[\"sigma_lower_bound\"]\n",
    "        self.sigma_mult = args[\"sigma_mult\"]\n",
    "        self.alpha = args[\"alpha\"]\n",
    "        self.alpha_mult = args[\"alpha_mult\"]\n",
    "        self.alpha_lower_bound = args[\"alpha_lower_bound\"]\n",
    "        self.gen_size = args[\"gen_size\"]\n",
    "        self.rewards = np.zeros(self.gen_size)\n",
    "        self.theta = np.zeros(self.theta_dim)\n",
    "        self.best_theta = np.zeros(self.theta_dim)\n",
    "        self.best_reward = -float(\"inf\") # lowest possible value\n",
    "        self.forget_best = args[\"forget_best\"]\n",
    "        self.weight_decay = args[\"weight_decay\"]\n",
    "        self.rank_fitness = args[\"rank_fitness\"]\n",
    "        if self.rank_fitness: # use rank rather than fitness numbers\n",
    "            self.forget_best = True # forget the best one if we use rank fitness\n",
    "\n",
    "    def ask(self):\n",
    "        '''returns a collection of model weights'''    \n",
    "        self.epsilon = np.random.randn(self.gen_size, self.theta_dim)\n",
    "        self.solutions = self.theta.reshape(1, self.theta_dim) + self.epsilon * self.sigma\n",
    "\n",
    "        return self.solutions\n",
    "\n",
    "    def tell(self, simulation_results):\n",
    "        '''updates internal variables based on the results from simulating the results from\n",
    "           self.ask '''\n",
    "\n",
    "        # input must be a numpy float array of the correct length\n",
    "        if len(simulation_results) != self.gen_size:\n",
    "            print \"incorrect length of input\"\n",
    "            return None\n",
    "\n",
    "        rewards = np.array(simulation_results)\n",
    "        \n",
    "        if self.rank_fitness:\n",
    "            rewards = compute_centered_ranks(rewards)\n",
    "        \n",
    "        # decay the weights of our neural network using\n",
    "        if self.weight_decay > 0:\n",
    "            l2_decay = compute_weight_decay(self.weight_decay, self.solutions)\n",
    "            rewards += l2_decay\n",
    "\n",
    "        index_ordering = np.argsort(rewards)[::-1]\n",
    "\n",
    "        best_reward = rewards[index_ordering[0]]\n",
    "        best_theta = self.solutions[index_ordering[0]]\n",
    "\n",
    "        self.curr_best_reward = best_reward\n",
    "        self.curr_best_theta = best_theta\n",
    "\n",
    "        # update our best parameters we are storing\n",
    "        if self.forget_best or (self.curr_best_reward > self.best_reward):\n",
    "            self.best_theta = best_theta\n",
    "            self.best_reward = self.curr_best_reward\n",
    "\n",
    "        # standardize the rewards to have a gaussian distribution\n",
    "        normalized_rewards = (rewards - np.mean(rewards)) / np.std(rewards)\n",
    "        change_theta = 1./(self.gen_size*self.sigma)*np.dot(self.epsilon.transpose(),\n",
    "                                                            normalized_rewards)\n",
    "        \n",
    "        self.theta += self.alpha * change_theta\n",
    "\n",
    "        # adapt sigma if not already too small\n",
    "        if (self.sigma > self.sigma_lower_bound):\n",
    "            self.sigma *= self.sigma_mult\n",
    "\n",
    "        # adapt learning rate if not already too small\n",
    "        if (self.alpha > self.alpha_lower_bound):\n",
    "            self.alpha *= self.alpha_mult\n",
    "\n",
    "    def current_param(self):\n",
    "        return self.curr_best_theta\n",
    "\n",
    "    def set_theta(self, theta):\n",
    "        self.theta = np.array(theta)\n",
    "\n",
    "    def result(self):\n",
    "        return self.best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Grace\n",
    "To exploit the parallel nature of the problem, I applied for an account on the high performance computing cluster, Grace. This allowed for a dramatic speedup of the training procedure of the policy, and eventually led to the solutions that are listed in the results section to the selected environments.  I adapted the parallel training procedure used by ESTool, the open source project written by the author of the blogpost I referenced.  This code used MPI as a way to communicate between multiple processes each running playouts of the environment simulation.  This parallel playout strucutre, which follows the procedure that OpenAI used in their paper, allows us to test many different random mutations of our current best performing paraments while only having to communicate the random seed used to generate the mutation noise.  This, of course, dramatically speeds up the amount of time each generation takes to complete.  I used the following run script on Grace \n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --partition=scavenge\n",
    "#SBATCH --job-name=bipedal_walker_hardcore_es\n",
    "#SBATCH --ntasks=64\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --mem-per-cpu=6000 \n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=email\n",
    "\n",
    "source activate amth\n",
    "pip install mpi4py\n",
    "python train.py -e 16 -n 64 -t 4 --start_file <model_start_file>\n",
    "```\n",
    "\n",
    "What this code script does is tells grace that we want to run on the scavenge partition, which means that we can use any hardware that is not currently being used by someone else; however, we can be prempted if anyone gets scheduled onto the hardware we got assigned to.  The reason that this is acceptable is that the training procedure that I adapted from ESTool allows for frequent checkpointing, so we can start where we left off whenever we get preempted. This means we don't have to wait in any fair-share queues unless there is no hardware available.  Next, this script tells Grace that we want 64 tasks for our MPI code and we want one process to be running in each of these tasks. \n",
    "\n",
    "The last three lines of the script simply switch to our relevant conda environment, which includes the open-ai gym, MPI, and other standard python packages that our code depends on. Additionally the last line of the script calls the training procedure which glues together the code that I presented in the report and spins up 64 MPI workers, each averaging their fitness score over 16 episodes (for robustness purposes) computing 4 trials per MPI worker. These were the recommended training settings from ESTool.  Lastly, optionally you can include the model start file, which rather than start from scratch every time, allows you to start at your previously found best parameters.  I implemented this functionality to allow the code to be run on the scavenge partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "For both of these environments, I wanted to make a point to use the same hyperparameters to demonstrate the true black-box opimization capabilities of evolutionary algorithms, in this case the modified version of OpenES described above. The hyperparameters for both of these solutions that I found were as described by this python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "args = {\"theta_dim\": model.num_flat_features(),\n",
    "        \"sigma_start\": .1,\n",
    "        \"sigma_mult\": .999,\n",
    "        \"sigma_lower_bound\": .02,\n",
    "        \"alpha\": .01,\n",
    "        \"alpha_mult\": 1,\n",
    "        \"alpha_lower_bound\": .01,\n",
    "        \"weight_decay\": .005,\n",
    "        \"start_sigma\": .05,\n",
    "        \"rank_fitness\": True, # use rank rather than fitness numbers\n",
    "        \"forget_best\": True,  # should we forget the best fitness at each iteration\n",
    "        \"gen_size\": gen_size\n",
    "       }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BipedalWalker-V2\n",
    "The results of this experiment were resoundingly positive.  I was able to train the normal bipedal walking task using the modified version of OpenES in around 2 hours and pass the stringent evaluation procedure of averaging 300 points over 100 trials.  Below are animations of two such solutions that I found\n",
    "\n",
    "![Trained Agent on BipedalWalker-v2](https://i.imgur.com/AAWxL3P.gif)\n",
    "\n",
    "### BipedalWalkerHardcore-V2\n",
    "As for the Hardcore environment, I was also pleased with the results that the modified version of OpenES achieved. Using the same algorithm and hyperparamaters that I used for the Normal Bipedal Walker environment, I was able to find a resonably robust agent that averaged around 220 points over the 300 trials. One thing to note is that I started my training of the hardcore environment with the solution that I found for the normal environment in an attempt to give the training procedure a leg up.  That being said, I ran the computation for around 48 hours, and it is possible that this average perforamce could have been better if I trained it for longer.  In addition to the difficulties that I described earlier about this environment, one tradeoff that I noticed was that the agent had to strike the right balance between its speed and its ability to not fall over.  If the agent is too slow, it is penalized for using too much motor torque to keep its balance for so long; however, if it goes too fast, it increases its likelihood to take a spill.  Below is a run of the best agent I was able to learn:\n",
    "\n",
    "![Trained Agent on BipedalWalkerHardcore-v2](https://i.imgur.com/W6xUQOT.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In my mind before I started the project and before I read OpenAI's paper, I had always seen evolutionary algorithms as cute black-box optimzation methods that did not have serious application in training modern machine learning models; however, as I found out firsthand by getting some spectacular results on these hard environments, these algorithms may actually have a place in the traditional reinforcement learning setting.  For parameter vectors that aren't incredibly large, such as in the millions and beyond, evolutionary algorithms offer an enticing option to train continuous control agents.  The ability to parallelize training on a CPU cluster offers a unique advantage to these methods, and people with access to a large number of cores might be able to rival reinforcement learning schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Ha, David. A Visual Guide to Evolution Strategies, 29 Oct. 2017, https://blog.otoro.net/2017/10/29/visual-evolution-strategies/.\n",
    "2. Ha, David. Evolving Stable Strategies, 12 Nov. 2017, http://blog.otoro.net/2017/11/12/evolving-stable-strategies/\n",
    "3. Salimans, T., Ho, J., Chen, X., Sidor, S., & Sutskever, I. (2017). Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864. \n",
    "4. Wikipedia Contributers. “CMA-ES.” Wikipedia, Wikimedia Foundation, 10 Apr. 2018, en.wikipedia.org/wiki/CMA-ES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Important Links\n",
    "ESTool:  https://www.github.com/hardmaru/estool \n",
    "\n",
    "OpenES repository:  https://github.com/openai/evolution-strategies-starter\n",
    "\n",
    "My full code: https://github.com/andrewmalta13/evolutionaryAlgorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
